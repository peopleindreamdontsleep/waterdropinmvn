spark {
  # Waterdrop defined streaming batch duration in seconds
  spark.streaming.batchDuration = 120

  # see available properties defined by spark: https://spark.apache.org/docs/latest/configuration.html#available-properties
  spark.master = "yarn-client"
  spark.app.name = "Waterdrop"
  spark.ui.port = 13000

  spark.executor.instances = 60
  spark.executor.cores = 2
  spark.executor.memory = "4g"
  spark.streaming.blockInterval= "1000ms"
  spark.streaming.kafka.maxRatePerPartition = 30000
  spark.streaming.kafka.maxRetries = 2
  spark.driver.extraJavaOptions = "-Dconfig.file=/data/slot6/waterdrop/application.conf"
}

input {
  kafka {
    topics = "gpc.oi_gaoyingju.test1"
    consumer.bootstrap.servers = "10.110.94.130:9092,10.110.94.131:9092,10.110.95.50:9092,10.110.95.68:9092,10.110.95.82:9092"
    consumer.zookeeper.connect = "10.110.94.130:2181,10.110.94.131:2181,10.110.95.50:2181,10.110.95.68:2181,10.110.95.82:2181"
    consumer.group.id = "scala_test_group1"
    consumer.num.consumer.fetchers = "4"
    consumer.auto.offset.reset = "largest"
  }
}

filter {
  split {
    # default delimeter is whitespace
    fields = ["time", "url", "http_status", "response_time", "refer", "body_size"]
  }

  date {
    format = "yyyyMMMddHHmmss"
    target_field = "dt"
  }

  if ${http_status} >= 500 {
    field {
      action = "add"
      field_name = "internal_error"
      value = 1
      overwrite = true
    }
  }

  # user defined plugin
  org.apache.mycompany.filters.pagerank {
  }
}

output {

	elasticsearch {
		hosts = ["10.11.110.45:9000", "10.11.110.46:9000"]
		index = "waterdrop-accesslog-${time}"
		thread = 3
	}

	if ${http_status} >= 400 AND ${http_status} < 500 {
		kafka {
			brokers = ["10.11.110.35:9092", "10.11.110.36:9092", "10.11.110.37:9092"]
			topic = "user_error"
		}
	}
}
